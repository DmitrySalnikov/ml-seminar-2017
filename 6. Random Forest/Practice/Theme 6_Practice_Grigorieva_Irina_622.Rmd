---
title: "Решающие деревья. Random Forest."
author: "Григорьева Ирина"
date: '2 октября 2017 г '
output: html_document
---

```{r}
#для деревьев классификации и регрессиии
library(tree) 
library(ISLR)
library(MASS)
```

####Для деревьев классификации и регрессиии используется алгоритм CART.

###Classification Trees

Carseats- набор данных, содержащий продажи детских автокресел в различных магазинах.

```{r}
attach(Carseats ) 
Carseats[1:10,] 
```

####Данные
#####400 наблюдений и 11 переменных:

Sales- количество проданных единиц (в тысячах) в каждом магазине,

CompPrice- цена, взимаемая конкурентом в каждом магазине,

Income- уровень дохода магазина (в тысячах долларов),

Advertising- рекламный бюджет (в тысячах долларов),

Population- размер населения в регионе (в тысячах),

Price- цена за автомобильное кресло магазина на каждом участке,

ShelveLoc- фактор с уровнями: Bad, Good, Medium (Плохо, Хорошо, Средний)- место расположения,

Age- средний возраст местного населения,

Education- уровень образования в каждом месте,

Urban- фактор с уровнями: No, Yes- указывает, находится ли магазин в городской или сельской местности,

US- фактор с уровнями: No, Yes- указывает, находится ли магазин в США или нет.

Sales - количественная переменная, представим ее  в виде двоичной переменной. Используем функцию $\mathrm{ifelse()}$ для создания переменной с именем High, которая принимает значение «Да», если переменная Sales превышает 8, и принимает значение «Нет» в противном случае.

```{r}
High<-ifelse(Sales <=8,"No","Yes")
Carseats<-data.frame(Carseats ,High)
Carseats[1:10,]
```

Используем функцию $\mathrm{tree()}$ для построения дерева классификации, чтобы прогнозировать High, используя все переменные, кроме Sales. 

Функция $\mathrm{summary()}$ перечисляет переменные, которые используются как внутренние узлы в дереве, считает количество терминальных узлов и коэффициент ошибок обучения.

```{r}
#Выращивается дерево путем двоичного разбиения
tree.carseats<-tree(High~.-Sales, Carseats) 
summary(tree.carseats)
```

Коэффициент ошибок классификации составляет $9\%=\frac{сумма~ошибочных~классификаций~в~листах}{число~наблюдений}*100\%$. 

Для деревьев классификации отклонение задается формулой $$-2\sum_{m} \sum_{k} n_{mk} \log{\hat{p}_{mk}},$$
где  $n_{mk}$ - число наблюдений в m-м терминальном узле, принадлежащих  k-му классу, $\hat{p}_{mk}$- доля наблюдений в m-м классе, принадлежащих k-му классу.

Остаточное среднее отклонение, указанное в выводе $\mathrm{summary()}$, представляет собой просто отклонение, деленное на $(n -число~терминальных~ узлов)$, которое в этом случае составляет 400-27 = 373.

Одним из наиболее привлекательных свойств деревьев является то, что они могут быть графически изображены. Используем функцию $\mathrm{plot()}$ для отображения древовидной структуры и функцию $\mathrm{text ()}$ для отображения меток узлов. Аргумент $\mathrm{pretty = 0}$ указывает, что необходимо отображать имена для качественных предикторов полностью, а не просто первую букву.

```{r}
plot(tree.carseats)
text(tree.carseats, pretty =0)
```

Самым важным показателем продаж (High), по-видимому, является место расположения (ShelveLoc), так как первое разделение отличает хорошие места от плохих и средних мест.

Если набрать имя объекта дерева, R дает вывод, соответствующий каждой ветви дерева. R отображает критерий разделения (например, цена <92,5), количество наблюдений в этой ветви, отклонение, общее предсказание для ветки (Yes или No) и доли наблюдений в этой ветви, которые принимают значения «Да» и «Нет». Ветви, которые приводят к терминальным узлам, обозначаются звездочками.

```{r}
tree.carseats
```

Чтобы правильно оценить эффективность дерева классификации этих данных, необходимо оценить тестовую ошибку, а не просто вычислять ошибку обучения.

Разделим наблюдения на тренировочный набор и тестовый набор, построим дерево с использованием набора для обучения и оценим его производительность по тестовым данным.

Для этой цели можно использовать функцию $\mathrm{predict()}$. В случае дерева классификации аргумент $\mathrm{type ="class"}$ инструктирует R возвращать фактическое предсказание класса. 

```{r}
set.seed(2) 
train<-sample(1: nrow(Carseats), 200) 
Carseats.test<-Carseats[-train, ]
High.test<-High[-train ]

tree.carseats<-tree(High~.-Sales, Carseats ,subset=train)
tree.pred<-predict(tree.carseats, Carseats.test, type ="class") #Возвращает вектор прогнозируемых ответов
table(tree.pred, High.test)

mean(High.test == tree.pred)
```

Этот подход приводит к правильным прогнозам примерно для 71.5% мест в наборе тестовых данных.

Посмотрим, приведет ли обрезка дерева к улучшению результатов. Функция $\mathrm{cv.tree()}$ выполняет кросс-валидацию, чтобы определить оптимальный уровень сложности дерева; Cost complexity pruning (слабая обрезка) используется для выбора последовательности деревьев для рассмотрения.

Используем аргумент $\mathrm{FUN = prune.misclass}$, чтобы указать, что хотим число ошибочных классификаций, чтобы управлять процессом обрезки, а не значение по умолчанию для функции $\mathrm{cv.tree()}$, которое является отклонением.

Функция $\mathrm{cv.tree ()}$ сообщает количество терминальных узлов каждого рассматриваемого дерева ($\mathrm{size}$), а также сумму ошибочных классификаций в каждом листе ($\mathrm{dev}$) и значение используемого параметра сложности $\alpha$- это компромисс между размером дерева и его соответствию данным ($\mathrm{k}$).


```{r}
set.seed(3)
cv.carseats<-cv.tree(tree.carseats, FUN=prune.misclass)
names(cv.carseats)
cv.carseats
```

Дерево с 9 терминальными узлами приводит к самой низкой частоте ошибок перекрестной проверки (50 ошибок).
Определяем частоту ошибок как функцию от размера $\mathrm{size}$ и параметра сложности $\mathrm{k}$.

```{r}
par(mfrow =c(1,2)) #для установки графических параметров
plot(cv.carseats$size, cv.carseats$dev, type="b")
plot(cv.carseats$k, cv.carseats$dev, type="b")
```

Теперь применим функцию $\mathrm{prune.misclass()}$, чтобы обрезать дерево и получить дерево с девятью узлами.

```{r}
prune.carseats<-prune.misclass(tree.carseats, best =9)
plot(prune.carseats)
text(prune.carseats, pretty =0)
```

Насколько хорошо это обрезанное дерево работает на тестовых данных? Применим функцию $\mathrm{predict()}$.

```{r}
tree.pred<-predict(prune.carseats, Carseats.test, type="class")
table(tree.pred, High.test)

mean(High.test == tree.pred)
```

Теперь 77% тестовых наблюдений правильно классифицированы, поэтому процесс обрезки дает не только более интерпретируемое дерево, но также улучшает точность классификации.
Если увеличить значение $\mathrm{best}$, получим более крупное обрезанное дерево с более низкой степенью точности классификации:

```{r}
prune.carseats<-prune.misclass(tree.carseats, best =15)
plot(prune.carseats)
text(prune.carseats, pretty =0)
tree.pred<-predict(prune.carseats, Carseats.test, type="class")
table(tree.pred, High.test)

mean(High.test == tree.pred)
```

###Bagging and Random Forests for classification

Применим bagging и random forests к данным Carseats, используя  пакет $\mathrm{randomForest}$.

Напоминание: bagging- это просто частный случай случайного леса с m = p, где m- используемое число признаков, а p- общее число предикторов. Поэтому функция $\mathrm{randomForest()}$ может использоваться для выполнения как random forests, так и bagging. 

Рассмотрим два варианта подсчета test error:

1)Построение деревьев на основе тренировочной выборки и подсчет ошибки на тестовой выборке.

2)Для оценки test error в методах bagging и случайный лес можно использовать OOB оценку, тогда нет необходимости делить выборку на тренировочную и тестовую.

##### Вариант 1)
Выполним bagging слудующим образом:

```{r}
library(randomForest)
set.seed(1)
bag.carseats<-randomForest(High~.-Sales, data=Carseats, subset =train, mtry=10, importance =TRUE)

bag.carseats
```

Насколько хорошо эта bagging модель работает на тестовом наборе?

```{r}
yhat.bag<-predict(bag.carseats, newdata=Carseats [-train ,])
table(pred =yhat.bag, true =High.test)

mean(High.test == yhat.bag)
```

Теперь 82% тестовых наблюдений правильно классифицированы. Следовательно, bagging  улучшает точность прогнозирования по сравнению с tree model. 

Можно изменить количество деревьев, выращенных методом $\mathrm{randomForest ()}$, используя аргумент $\mathrm{ntree}$. Количество деревьев не является критическим параметром при использовании бэггинга: очень большое значение не приведет к переобучению. На практике обычно используется значение $\mathrm{ntree}$ достаточно большое для стабилизации ошибки (по умолчанию, $\mathrm{ntree}$=500).

Выращивание случайного леса происходит точно так же, за исключением того, что используется меньшее значение аргумента mtry.

По умолчанию $\mathrm{randomforest ()}$ использует $\frac{p}{3}$ переменных  при построении случайного леса деревьев регрессии и $\sqrt{p}$ переменных при построении случайного леса деревьев классификации.

Выполним random forest следующим образом: 

```{r}
set.seed(1)
rf.carseats<-randomForest(High~.-Sales, data=Carseats, subset=train, mtry=6, importance =TRUE)

yhat.rf<-predict(rf.carseats, newdata=Carseats[-train ,])
table(pred =yhat.rf, true =High.test)

mean(High.test == yhat.rf)
```

Уровень ошибочной классификации не изменился по сравнению с bagging. 82% тестовых наблюдений правильно классифицированы.

##### Вариант 2)
Выполним bagging следующим образом:

```{r}
library(randomForest)
set.seed(1)
bag.carseats<-randomForest(High~.-Sales, data=Carseats, mtry=10, importance =TRUE)

bag.carseats

#OOB estimate of  error rate (оценка тестовой ошибки)
(28+50)/400
```

Вышеуказанный коэффициент ошибок классификации вычисляется с использованием оценки Out-of-Bag Error:
$\frac{2}{3}$ данных используются для обучения, а остальные $\frac{1}{3}$ (OOB) используются для проверки деревьев. 

Теперь 80.5% наблюдений правильно классифицированы. Следовательно, bagging  улучшает точность прогнозирования по сравнению с tree model. 

Выполним random forest следующим образом: 

```{r}
set.seed(1)
rf.carseats<-randomForest(High~.-Sales, data=Carseats, mtry=3, importance =TRUE)

rf.carseats

#OOB estimate of  error rate (оценка тестовой ошибки)
(26+49)/400
```

Уровень ошибочной классификации немного уменьшился по сравнению с bagging. 81.5% тестовых наблюдений правильно классифицированы.

Оценим важность каждой переменной:

```{r}
importance(rf.carseats)
```

Два измерения важности переменной:

1) MeanDecreaseAccuracy- cреднее снижение точности (коэффициент ошибок для классификации).

2) MeanDecreaseGini- cреднее падение индекса Джини. При каждом ветвлении на дереве падает индекс Джини. Для каждой переменной можно посчитать суммарное падение индекса Джини, вызванное ветвлениями на базе этой переменной. Посчитав среднее падение Джини по всем деревьям получим меру важности.

Графики этих мер важности:

```{r}
varImpPlot(rf.carseats)
```

Результаты показывают, что на всех деревьях, рассматриваемых в случайном лесу, место расположения магазина (ShelveLoc) и стоимость детского автомобильного кресла (Price) являются двумя наиболее важными переменными.

###Regression Trees

Рассмотрим набор данных Boston: стоимость жилья в пригородах Бостона.

```{r}
Boston[1:10,]
```

####Данные
#####506 наблюдений и 14 переменных:

Сrim- уровень преступности в городе на душу населения,

Zn- доля жилой земли, разделенной на участки более 25 000 кв. футов,

Indus- доля земли в городе, не подлежащая продаже,

Chas- фиктивная переменная Charles River: 1- если река пересекает земельный участок, 0- в противном случае,

Nox- концентрация оксидов азота (частей на 10 миллионов),

Rm- среднее количество комнат жилья,

Age- доля участвов, заселенных до 1940 года,

Dis- взвешенное среднее расстояний до пяти центров занятости в Бостоне,

Rad- индекс доступности радиальных магистралей,

Tax- налог на имущество (в 10 000 долларов),

Ptratio- соотношение учеников и учителей по городу,

Black- $1000*(B_k-0.63)^2$, где $B_k$- доля чернокожих в городе,

Lstat- низкий статус населения (в процентах),

Medv- медианная стоимость домов, занятых владельцами (в 1000 долларов).

Создадим обучающий набор и построим дерево с использованием набора для обучения. Используем функцию $\mathrm{tree()}$ для построения дерева регрессии.

```{r}
set.seed(1)
train<-sample(1: nrow(Boston ), nrow(Boston )/2)

#Выращивается дерево путем двоичного разбиения
tree.boston<-tree(medv~.,Boston, subset =train)
summary(tree.boston )

#total residual deviance
sum(sapply(resid(tree.boston),function(x)(x-mean(resid(tree.boston)))^2))
```

Обратите внимание, что вывод $\mathrm{summary ()}$ указывает, что для построения дерева использовались только три из переменных. В выводе отображается количество терминальныъ узлов, остаточное среднее отклонение, общее остаточное отклонение- это просто сумма квадратов ошибок для дерева в контексте дерева регрессии. Построим дерево:

Используем функцию $\mathrm{plot()}$ для отображения древовидной структуры и функцию $\mathrm{text()}$ для отображения меток узлов.

```{r}
plot(tree.boston)
text(tree.boston)
```

Дерево указывает, что маленькие значения lstat (процент лиц с низким социально-экономическим статусом) соответствуют более дорогим домам. Дерево предсказывает, что стоимость дома в размере 46 400 долларов соответствует более крупным домам в пригородах, где жители имеют высокий социально-экономический статус (rm $\ge$ 7.437 и lstat <9.715).

Теперь используем функцию $\mathrm{cv.tree ()}$, чтобы увидеть, улучшит ли дерево pruning. Функция $\mathrm{cv.tree()}$ выполняет кросс-валидацию, чтобы определить оптимальный уровень сложности дерева; Cost complexity pruning (слабая обрезка) используется для выбора последовательности деревьев для рассмотрения.

Функция $\mathrm{cv.tree ()}$ сообщает количество терминальных узлов каждого рассматриваемого дерева ($\mathrm{size}$), а также отклонение ($\mathrm{dev}$) и значение используемого параметра сложности $\alpha$- это компромисс между размером дерева и его соответствию данным ($\mathrm{k}$). Для регресии $\mathrm{dev = RSS}$.

```{r}
cv.boston<-cv.tree(tree.boston)

cv.boston

plot(cv.boston$size, cv.boston$dev, type='b')  
```


Мы могли бы сделать обрезку дерева, используя функцию $\mathrm{prune.tree ()}$, но по графику видно, что 
дерево с 8 терминальными узлами приводит к самой низкой сумме квадратов ошибок для дерева. Следовательно, обрезка не требуется.

Используем unpruned дерево, чтобы сделать прогнозы на тестовом наборе. Для этой цели можно использовать функцию $\mathrm{predict()}$:

```{r}
yhat<-predict(tree.boston, newdata=Boston[-train ,])
boston.test<-Boston [-train ,"medv"]

#График наблюдаемых значений (по оси x) и прогнозируемых значений (ось y)
plot(yhat, boston.test) 

# Добавим диагональную линию
abline(0,1)

# Вычисление MSE тестового набора
mean((yhat-boston.test)^2)

```

MSE тестового набора, связанная с деревом регрессии, равна 25.05. Таким образом, квадратный корень из MSE составляет около 5.005, что указывает на то, что эта модель приводит к предсказаниям тестового набора, которые отклоняются на $ 5.005 от истинной медианной стоимости домов в пригороде (medv).

###Bagging and Random Forests for regression

Применим bagging и random forests к данным Бостона, используя  пакет $\mathrm{randomForest}$.

Рассмотрим два варианта подсчета test error:

1)Построение деревьев на основе тренировочной выборки и подсчет ошибки на тестовой выборке.

2)Для оценки test error в методах bagging и случайный лес можно использовать OOB оценку, тогда нет необходимости делить выборку на тренировочную и тестовую.

##### Вариант 1)
Выполним bagging слудующим образом:

```{r}
library(randomForest)
set.seed(1)
bag.boston<-randomForest(medv~., data=Boston, subset =train, mtry=13, importance =TRUE)

bag.boston
```

Аргумент $\mathrm{mtry = 13}$ указывает на то, что для каждого разбиения дерева следует учитывать все 13 предикторов.

Насколько хорошо эта bagging модель работает на тестовом наборе?

```{r}
yhat.bag<-predict(bag.boston, newdata=Boston [-train ,])

#График наблюдаемых значений (по оси x) и прогнозируемых значений (ось y)
plot(yhat.bag, boston.test)
abline(0,1)

mean((yhat.bag-boston.test)^2)
```

MSE тестового набора, связанная с bagged regression tree, составляет 13.47, что почти вдвое меньше, чем полученное с использованием оптимального обрезанного дерева.

Можно изменить количество деревьев, выращенных методом $\mathrm{randomForest ()}$, используя аргумент $\mathrm{ntree}$. Количество деревьев не является критическим параметром при использовании бэггинга: очень большое значение не приведет к переобучению. На практике обычно используется значение $\mathrm{ntree}$ достаточно большое для стабилизации ошибки (по умолчанию, $\mathrm{ntree}$=500).

```{r}
plot(bag.boston)
```

Выполним random forest следующим образом:

```{r}
set.seed(1)
rf.boston<-randomForest(medv~., data=Boston, subset=train, mtry=6, importance =TRUE)
yhat.rf<-predict(rf.boston, newdata=Boston[-train ,])

mean((yhat.rf -boston.test)^2)
```

MSE тестового набора составляет 11.48. В этом случае случайные леса дали улучшение по сравнению с bagging.

##### Вариант 2)
Выполним bagging слудующим образом:

```{r}
library(randomForest)
set.seed(1)
bag.boston<-randomForest(medv~., data=Boston, mtry=13, importance =TRUE)

bag.boston
```

Вышеуказанные MSE вычисляется с использованием оценки Out-of-Bag Error.
$\frac{2}{3}$ данных используются для обучения, а остальные $\frac{1}{3}$ (OOB) используются для проверки деревьев. 

Аргумент $\mathrm{mtry = 13}$ указывает на то, что для каждого разбиения дерева следует учитывать все 13 предикторов.

OOB MSE, связанная с bagged regression tree, составляет 10.38, что гораздо меньше, чем полученное с использованием tree model.
 
Выполним random forest следующим образом:

```{r}
set.seed(1)
rf.boston<-randomForest(medv~., data=Boston, mtry=6, importance =TRUE)

rf.boston
```

OOB MSE составляет 9.76. В этом случае случайные леса дали улучшение по сравнению с bagging.

Оценим важность каждой переменной (чем выше число, тем признак важнее):

```{r}
importance(rf.boston )
```

Два измерения важности переменной: 

1) %IncMSE- среднее снижении точности прогнозов (оценивается MSE предсказаний). 

2) IncNodePurity - мера среднего увеличения “чистоты узла” дерева (node purity) в результате разбиения данных по соответствующей переменной. В случае деревьев регрессии чистота узлов измеряется RSS обучающего набора.

Графики этих мер важности:

```{r}
varImpPlot (rf.boston )
```


Результаты показывают, что на всех деревьях, рассматриваемых в случайном лесу, уровень богатства населения (lstat) и размер дома (rm) являются двумя наиболее важными переменными.













